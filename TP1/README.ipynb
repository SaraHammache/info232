{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"logo.jpg\", width=150, ALIGN=\"left\">\n",
    "<center>\n",
    "<h1>Mini Projets 2019 (Info 232)</h1>\n",
    "Isabelle Guyon <br>\n",
    "info232@chalearn.org <br>\n",
    "</center>\n",
    "<span style=\"color:red\"> <h1> 1. Visualisation </h1> </span>\n",
    "    \n",
    "<p> We are visualizing the famous <a href=\"https://archive.ics.uci.edu/ml/datasets/iris\"> Iris dataset </a>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h2>Instructions </h2>\n",
    "    <p>\n",
    "        Ce TP vaut 5 points. Repondez a toutes les questions. <br>\n",
    "    <ul>\n",
    "        <li> Pour creer une nouvelle cellule, allez dans le menu \"Insert\".</li>\n",
    "        <li> Pour transformer une cellule en commentaire texte, allez dans Cell + Cell Type + Markdown. </li>\n",
    "        <li> Pour executer une cellule: SHIFT+RETURN </li>\n",
    "    </ul>\n",
    "    </p>\n",
    "    <p> Les cellules doivent entre executee dans l'ordre. Pour plus d'information <a href\"https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/What%20is%20the%20Jupyter%20Notebook.html\"> CONSULTEZ la DOCUMENTATION. </a>\n",
    "            \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 0: \"Markdown\" cells\n",
    "\n",
    "Creez une nouvelle cellule de type Markdown en dessous de celle-ci.\n",
    "Recopiez dedans le paragraphe sur k-fold cross validation que vous trouverez sur Wikipedia. Essayez de colorer la cellule en VERT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La validation croisée (« cross-validation ») est, en apprentissage automatique, une méthode d’estimation de fiabilité d’un modèle fondé sur une technique d’échantillonnage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: \"Code\" cells\n",
    "\n",
    "Maintenant vous allez executer la cellule ci-dessous apres avoir replace la reponse par 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"background:#00FF00\">CORRECT<br>:-)</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "code_dir = 'code/'                        \n",
    "from sys import path; path.append(code_dir)\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from checker import check\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "question = 1\n",
    "answer = 1  # Replace by 1\n",
    "score = 0\n",
    "score += check(answer, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: AutoML format\n",
    "\n",
    "Les donnees que vous aurez a analyser dans votre projet seront au <a href=\"https://github.com/codalab/chalab/wiki/Help:-Wizard-%E2%80%90-Challenge-%E2%80%90-Data\">format AutoML</a>:\n",
    ".\n",
    "En utilisant les moyens que vous voulez, remplacez les valeurs des variables de dimension des donnees par leur valeurs correctes dans la deuxieme cellule avant de l'executer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_feat.name\t   iris_public.info    iris_train.data\t    iris_valid.solution\r\n",
      "iris_label.name    iris_test.data      iris_train.solution\r\n",
      "iris_private.info  iris_test.solution  iris_valid.data\r\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'             \n",
    "data_name = 'iris'\n",
    "!ls $data_dir*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"background:#00FF00\">CORRECT<br>:-)</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_number = 4         # put correct value\n",
    "training_sample_number = 35  # put correct value\n",
    "validation_sample_number = 35 # put correct value\n",
    "test_sample_number = 35       # put correct value\n",
    "question = 2\n",
    "reponse = feature_number*(training_sample_number+validation_sample_number+test_sample_number)\n",
    "score += check(reponse, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Pandas\n",
    "\n",
    "En Anglais pour changer :-) \n",
    "\n",
    "This time we are going to do simple \"exploratory data analysis\". To simplify, we lump all the data together in one big data structure calle a \"pandas\" data frame. This will allow us to use the rich libraries \"pandas\" and \"seaborn\" to explore the data. \n",
    "\n",
    "In the next cell, replace the \"head\" function, which just shows the first few rows of the dataset, by a padas function providing descriptive statistics. To that end, you may want to check the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/frame.html\">Pandas DataFrame reference page</a>. Then, in the following cell, replace the variables with their correct values and execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from data_io import read_as_df\n",
    "data = read_as_df(data_dir  + '/' + data_name)                # The data are loaded as a Pandas Data Frame\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_sepal_length = 0.892565 # Standard deviation of the sepal length, put correct value\n",
    "mean_sepal_width = 3.005714   # Mean of the sepal width, put correct value\n",
    "min_petal_length = 1.000000  # Minimum value of the petal length, put correct value\n",
    "max_petal_width = 2.500000   # Maximum value of the petal width, put correct value\n",
    "question = 3\n",
    "reponse = std_sepal_length+mean_sepal_width+min_petal_length+max_petal_width\n",
    "score += check(reponse, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Histograms\n",
    "\n",
    "Un truc sympa de Pandas c'est que ca permet aussi de faire des graphes pour visualiser les donnees. Remplacez le nombre de bins par un plus petit nombre. Repondez en changeant la variable answer dans la deuxieme cellule: answer=1 si la hauteur de la plus grande barre dimininue et answer=0 sinon. Comprenez-vous pourquoi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(figsize=(10, 10), bins=10, layout=(3, 2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 4\n",
    "answer = 1          # 1 if the maximum bar height increases when the bin number decreses, 0 otherwise\n",
    "score += check(answer, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have fun with the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/frame.html\">Pandas DataFrame reference page</a>. There are lots of other plotting functions. Try some of them in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's this one for instance? Change it to something else.\n",
    "data.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Pair plots\n",
    "\n",
    "Seaborn (sns) is a package of data visualization functions: https://seaborn.pydata.org/. Quite useful! It is convenient to visualize data in 2 dimensions. One way of doing that is to plot a variable (feature) against another one, one point representing a sample (a flower). The pairplot function shows all the possibilities (off-diagonal graphs). \n",
    "\n",
    "On the diagonal, what do you see? Compare with the histograms of the previous question.\n",
    "Then add another argument to the pairplot function \n",
    "        \n",
    "        hue=\"target\" \n",
    "        \n",
    "(if you do not understand, consult the documentation https://seaborn.pydata.org/generated/seaborn.pairplot.html). After executing the next cell again, in the folowing cell, answer the questions: \n",
    "\n",
    "What is the color of the class, which is best separated from all others?\n",
    "\n",
    "        color_best_separated = 1 if blue; 2 if orange; 3 if green.\n",
    "\n",
    "Which iris type does this correspond to?\n",
    "\n",
    "        iris_best_separated = 1 if virginica; 2 if versocolor; 3 if setosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "sns.pairplot(data, hue=\"target\"), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 5\n",
    "color_best_separated = 1 # Change that\n",
    "iris_best_separated = 3 # Change that\n",
    "score += check(color_best_separated*iris_best_separated, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Feature correlation\n",
    "Le variables (features) peuvent etre redondantes (c'est a dire capturer des informations similaires). Le coefficient de correlation de Pearson (voir <a href =\"https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\">page Wikipedia</a>) permet de detecter la correlation (c'est a dire la similarite au sens d'une dependance lineaire). \n",
    "\n",
    "En regardant les \"pair plots\" ci-dessus, a votre avis, quelle paire de variable est la plus correlee? Reperez la paire par son numero de ligne et de colonne pour repondre. Verifiez votre intuition en executant la cellule suivante qui represente graphiquement la matrice de correlation. Changez la methode 'pearson' pour d'autres coefficients de correlation (voir <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\">documentation</a>). Est-ce que ca change quelle paire est la plus correlee? Regardez les definitions de <a href =\"Kandall tau\">https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient</a> et <a href=\"https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient\">Spearman correlation coefficient</a>. Essayez de comprendre la difference entre <a href=\"https://en.wikipedia.org/wiki/Correlation_and_dependence\">correlation et dependence</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 6\n",
    "numero_ligne = 2             # Lignes numerotee de 0 a 3, remplacer la reponse\n",
    "numero_colonne = 3            # Colonnes numerotee de 0 a 3, remplacer la reponse\n",
    "score += check(numero_ligne*numero_colonne, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_mat = data.corr(method='pearson')\n",
    "sns.heatmap(corr_mat, annot=True, center=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarquez que les variables peuvent etre correlee ou anti-corelees. Vous voulez donc peut-etre vous interesser a la valeur absolue du coefficient de correlation. Cela change-t-il votre reponse? Peut-etre pas, mais ca pourrait!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(abs(corr_mat), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que la matrice est symetrique. Si vous voulez vous amuser, effacez les valeurs au dessus de la diagonale (<a href=\"https://seaborn.pydata.org/generated/seaborn.heatmap.html\">voir en bas de cette page</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: Feature selection\n",
    "Representing a matrix of coefficients with colors seems to be pretty convenient for visualization purposes. We would like to do that also for the data matrix itself. Note that, since the last column (target) contains strings (\"categorical variables\"), we first need to convert them to numbers. \n",
    "\n",
    "Observing the heatmap, which column is most correlated with the target? Insert another cell in which you plot the correlation matrix of data_new (inspiring yourself from the previous question), then confirm your intuition and anwer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())\n",
    "data_num = data.copy()  # If you don't use \"copy\", any change in data_num will also result in a change in data\n",
    "data_num['target']= data_num['target'].astype('category')\n",
    "data_num['target'] = data_num['target'].cat.codes\n",
    "print(data_num.head())\n",
    "sns.heatmap(data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = data_num.corr(method='pearson')\n",
    "sns.heatmap(corr_mat, annot=True, center=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La question 7 est ici:\n",
    "Quelle est la variable (feature) la plus correlee avec la colonne \"target\"? Quelle est la valeur du coefficient de correlation de Pearson correspondant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 7\n",
    "numero_variable = 2           # Variables numerotees de 0 a 3, replacez la reponse\n",
    "pearson_correlation = 0.97     # Mettre la valeur correcte\n",
    "score += check(numero_variable+pearson_correlation, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8: One-Rule classifier\n",
    "In this section, we show how we can create a very simple classifier based on just ONE rule to separate the 3 types of flowers. That rule classifies irises on the basis of their petal length only. Check the code to see whether you understand it.\n",
    "\n",
    "This classifier respects the structure of <a href =\"https://scikit-learn.org/stable/\">scikit-learn</a> learning machines. To make it compatible with other scikit-learn tools, we derive it from the base class BaseEstimator and overload 2 methods: \"fit\" and \"predict\". Then we use the Iris data to train and test a model (in this case we lumped all the data into a single matrix and use it as training data). After that we compute the training error.\n",
    "\n",
    "Scikit-learn allows you to compute <a href =\"https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics\"> a lot of other metrics </a>. To answer this question, you will have to compute the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score\">BAC</a>, that is the Balanced ACcuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class oneR(BaseEstimator):\n",
    "    ''' One Rule classifier '''\n",
    "    def __init__(self):\n",
    "        ''' The \"constructor\" initializes the parameters '''\n",
    "        self.selected_feat = 0 \t# The chosen variable/feature\n",
    "        self.theta1 = 0 \t\t# The first threshold\n",
    "        self.theta2 = 0\t\t\t# The second threshold\n",
    "\n",
    "    def fit(self, X, Y, F=[]):\n",
    "        ''' The method \"fit\" trains a super-simple classifier '''\n",
    "        if not F: F=[str(item) for item in range(X.shape[1])]\n",
    "        # First it selects the feature most correlated to the target\n",
    "        correlations = np.corrcoef(X, Y, rowvar=0)\n",
    "        self.selected_feat = np.argmax(correlations[0:-1, -1])\n",
    "        best_feat = X[:, self.selected_feat]\n",
    "        print('Feature selected = ' +  F[self.selected_feat])\n",
    "        # Then it computes the average values of the 3 classes\n",
    "        mu0 = np.median(best_feat[Y==0])\n",
    "        mu1 = np.median(best_feat[Y==1])\n",
    "        mu2 = np.median(best_feat[Y==2])\n",
    "        # Finally is sets two decision thresholds\n",
    "        self.theta1 = (mu0+mu1)/2.\n",
    "        self.theta2 = (mu1+mu2)/2.\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' The method \"predict\" classifies new test examples '''\n",
    "        # Select the values of the correct feature\n",
    "        best_feat = X[:, self.selected_feat]\n",
    "        # Initialize an array to hold the predicted values\n",
    "        Yhat = np.copy(best_feat)\t\t\t\t# By copying best_fit we get an array of same dim\n",
    "        # then classify using the selected feature according to the cutoff thresholds\n",
    "        Yhat[best_feat<self.theta1] = 0\t\t\t\t\t\t\t\t\t\t\t# Class 0\n",
    "        Yhat[np.all([self.theta1<=best_feat, best_feat<=self.theta2], 0)] = 1\t# Class 1\n",
    "        Yhat[best_feat>self.theta2] = 2 \t\t\t\t\t\t\t\t\t\t# Class 2\n",
    "        return Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY=data_num.as_matrix()\t\t\t\t# On transforme le Pandas dataframe en un Numpy array\n",
    "X = XY[:,0:4]\t\t\t\t\t\t# On recupere X (num_exemples x num_features)\n",
    "Y = XY[:,4]\t\t\t\t\t\t\t# et Y (num_exemples x 1) ==> les valeurs de target\n",
    "feature_names = list(data_num)[:-1] # On recupere aussi les noms des features\n",
    "my_model = oneR()\n",
    "my_model.fit(X, Y, feature_names)\t# Le nom des features est \"optionnel\", il peut etre supprime\n",
    "Yhat = my_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(solution, prediction):\n",
    "    return np.mean(solution!=prediction)\n",
    "    \n",
    "errate = error_rate(Y, Yhat)\n",
    "print('Training error = %5.2f' % errate)\n",
    "Yperm = np.random.permutation(Y)\n",
    "print('Random permutation error (for comparison)= %5.2f' % error_rate(Y, Yperm))\n",
    "print('Ideal error rate (for comparison)= %5.2f' % error_rate(Y, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La question 8 est ici:\n",
    "Remplacez xxx par la bonne function qui calcule le <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score\">BAC</a>, i.e. le Balanced ACcuracy score, et enlevez les # de commentaire. Observez que error_rate est different de Balanced Error Rate BER=1-BAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BER = errate\n",
    "from sklearn.metrics import accuracy_score\n",
    "BAC = accuracy_score(Y, Yhat)\n",
    "print('BAC = %5.2f' % BAC)\n",
    "BER = 1 - BAC\n",
    "print('BER = %5.2f' % BER)\n",
    "question = 8\n",
    "score += check(BER, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9: Viewing the classification results\n",
    "Pour se faire une idee de comment marche les classifieurs il est utile de visualiser les regions de decision. Ce petit programme permet de le faire en deux dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors = [(1, 0, 0), (0, 1, 0), (0, 0, 1)] # Red, lime, blue\n",
    "cm = LinearSegmentedColormap.from_list('rgb', colors, N=3)\n",
    "\n",
    "def ClfScatter(clf, X, Y, F, dim1=0, dim2=1, title=''):\n",
    "    '''clf_scatter(clf, X, Y, F, dim1=0, dim2=1)\n",
    "    Display decision function and training examples.\n",
    "    clf: a classifier with at least a fit and a predict method\n",
    "    like a sckit-learn classifier.\n",
    "    X: a 2 dimensional data matrix, samples in line and features in columns\n",
    "    Y: a target vectors of class values 0, 1, 2, etc.\n",
    "    F: feature names\n",
    "    dim1 and dim2: chosen features.\n",
    "    title: Figure title.\n",
    "    Returns: Predictions on training examples.\n",
    "    '''\n",
    "    # Fit model in chosen dimensions\n",
    "    X2 = X[:,(dim1,dim2)]\n",
    "    try:\n",
    "        clf.fit(X2, Y, F=[F[dim1],F[dim2]])\n",
    "    except:\n",
    "        clf.fit(X2, Y)\n",
    "    # Define a mesh    \n",
    "    x_min, x_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1\n",
    "    y_min, y_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1\n",
    "    h = 0.1 # step\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Xtest = np.c_[xx.ravel(), yy.ravel()]\n",
    "    # Compute the training error\n",
    "    Yhat = clf.predict(X2) \n",
    "    training_error = error_rate(Y, Yhat)\n",
    "    # Make your predictions on all mesh grid points (test points)\n",
    "    Yhat = clf.predict(Xtest) \n",
    "    # Make contour plot for all points in mesh\n",
    "    Yhat = Yhat.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Yhat, cmap=plt.cm.Paired)\n",
    "    # Overlay scatter plot of training examples\n",
    "    plt.scatter(X2[:, 0], X2[:, 1], c=Y, cmap=cm)   \n",
    "    plt.title('{}: training error = {:5.2f}'.format(title, training_error))\n",
    "    plt.xlabel(F[dim1])\n",
    "    plt.ylabel(F[dim2])\n",
    "    plt.show()\n",
    "    return clf.predict(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat = ClfScatter(my_model, X, Y, feature_names, dim1=2, dim2=3, title='OneR')\n",
    "errate = error_rate(Y, Yhat)\n",
    "print('Training error = %5.2f' % errate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La question 9 est ici:\n",
    "Replacez dim1=2, dim2=3 par dim1=0, dim2=1 dans la cellule du dessus et re-executez la. Vous devriez obtenir de moins bonnes performances. Pensez vous qu'un classifieur qui utiliserait plusieurs regles pourrait obtenir 0 erreurs?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 9\n",
    "reponse = 1 # Oui = 1, Non = 0\n",
    "score += check(errate+reponse, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10: More classifiers\n",
    "En utilisant le code precedant, on peut comparer plein de classifieurs. Certains n'obtiennent pas les memes resultats si on les fait tourner plusieurs fois, d'autres donnent toujours la meme chose. Combien d'entre eux (parmi les exemples ci-dessous) donnent toujours la meme chose? Verifiez bien dans la doc de <a href =\"https://scikit-learn.org/stable/\">scikit-learn</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "classifiers = [\n",
    "    Perceptron(random_state=random.randint(1,101)),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    GaussianNB(),\n",
    "    DecisionTreeClassifier(random_state=random.randint(1,101)),\n",
    "    SVC()]\n",
    "names = ['Perceptron',\n",
    "         'Linear Discriminant Analysis', \n",
    "         'Gaussian Classifier', \n",
    "         'Decision Tree', \n",
    "         'Support Vector Machine']\n",
    "for clf, name in zip(classifiers, names):\n",
    "    # This does a two dimensional fit in dim1 and dim2\n",
    "    ClfScatter(clf, X, Y, feature_names, dim1=2, dim2=3, title=name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 10\n",
    "reponse = 0 # Nombre de classifieurs qui retournent toujours la meme separation des classes; changer\n",
    "score += check(reponse, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Your final score is %d / 10, congratulations!' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
